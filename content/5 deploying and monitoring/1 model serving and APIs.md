---
title: Model Serving and APIs
date: 2025-10-02
---

Model serving provides a way to make trained neural networks accessible for real-world use. Serving frameworks such as TensorFlow Serving, TorchServe, and ONNX Runtime allow models to be deployed efficiently across different environments. Once deployed, models are often exposed through REST or gRPC APIs, enabling other applications to send inputs and receive predictions. Depending on the use case, inference can be performed in batch mode for large datasets or in real-time for applications requiring immediate responses, such as chatbots or fraud detection systems.





### Navigation
[[index|Back to Main Categories]]
[[2 containerization and orchestration|Next: Containerization and Orchestration]]