---
title: Feedforward Architectures
date: 2025-10-02
---

Feedforward neural networks, often called multilayer perceptrons (MLPs), are the simplest neural architectures. Their structure is composed of sequential layers of neurons, where each neuron in one layer connects to every neuron in the next layer. Information moves strictly forward, from the input layer, through one or more hidden layers, to the output layer without loops or feedback. Nonlinear activation functions between layers allow the network to approximate complex functions. The fully connected structure makes FNNs highly expressive but computationally heavy, serving as the fundamental building block for many more advanced architectures.






### Navigation
[[index|Back to main categories]]
[[02 convolutional architectures|Next: Convolutional Architectures]]