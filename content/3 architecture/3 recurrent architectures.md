---
title: Recurrent Architectures
date: 2025-10-02
---

Recurrent neural networks extend the feedforward design by adding feedback loops within their architecture, allowing information from previous time steps to influence current outputs. This recurrent structure provides a form of memory, making RNNs suitable for sequential data. However, standard RNNs struggle with long-term dependencies due to vanishing and exploding gradients. Variants like Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs) modify the architecture with gating mechanisms that regulate the flow of information, improving stability over longer sequences. More recently, transformer architectures have replaced recurrence with self-attention layers, enabling parallel processing of sequences and more effective capture of global dependencies.




### Navigation
[[index|Back to Main Categories]]
[[2 convolutional architectures|Previous: Convolutional Architectures]]